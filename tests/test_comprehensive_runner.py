#!/usr/bin/env python3
"""
Comprehensive test runner for Gmail Fetcher suite.
Validates all generated tests and provides detailed reporting.
"""

import pytest
import subprocess
import sys
import json
import time
from pathlib import Path
import importlib.util


class TestValidationRunner:
    """Test runner that validates all generated tests work correctly."""
    
    def setup_method(self):
        """Setup test validation environment."""
        self.test_files = [
            "test_core_gmail_assistant.py",
            "test_parsers_advanced_email.py", 
            "test_cli_main_orchestrator.py",
            "test_classification_analysis.py"
        ]
        
        self.test_results = {}
        
    def test_all_test_files_exist(self):
        """Verify all generated test files exist and are readable."""
        tests_dir = Path(__file__).parent
        
        for test_file in self.test_files:
            test_path = tests_dir / test_file
            assert test_path.exists(), f"Test file {test_file} not found"
            assert test_path.is_file(), f"{test_file} is not a file"
            assert test_path.stat().st_size > 0, f"{test_file} is empty"
            
            print(f"âœ… {test_file} exists ({test_path.stat().st_size} bytes)")
    
    def test_python_syntax_validation(self):
        """Validate Python syntax in all test files."""
        tests_dir = Path(__file__).parent
        
        for test_file in self.test_files:
            test_path = tests_dir / test_file
            
            try:
                # Compile the file to check syntax
                with open(test_path, 'r', encoding='utf-8', errors='ignore') as f:
                    source = f.read()
                
                compile(source, str(test_path), 'exec')
                print(f"âœ… {test_file} has valid Python syntax")
                
            except SyntaxError as e:
                pytest.fail(f"Syntax error in {test_file}: {e}")
            except Exception as e:
                pytest.fail(f"Error validating {test_file}: {e}")
    
    def test_import_validation(self):
        """Validate that all test files can be imported successfully."""
        tests_dir = Path(__file__).parent
        
        for test_file in self.test_files:
            test_path = tests_dir / test_file
            module_name = test_file[:-3]  # Remove .py extension
            
            try:
                spec = importlib.util.spec_from_file_location(module_name, test_path)
                if spec and spec.loader:
                    module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(module)
                    print(f"âœ… {test_file} imports successfully")
                else:
                    print(f"âš ï¸  {test_file} could not create import spec")
                    
            except ImportError as e:
                print(f"âš ï¸  Import error in {test_file}: {e}")
                # Don't fail test for import errors due to missing dependencies
            except Exception as e:
                pytest.fail(f"Error importing {test_file}: {e}")
    
    def test_pytest_discovery(self):
        """Test that pytest can discover all test functions."""
        tests_dir = Path(__file__).parent
        
        for test_file in self.test_files:
            test_path = tests_dir / test_file
            
            try:
                # Run pytest discovery only
                result = subprocess.run([
                    sys.executable, "-m", "pytest", 
                    str(test_path), "--collect-only", "-q"
                ], 
                capture_output=True, text=True, timeout=30
                )
                
                if result.returncode == 0:
                    # Count discovered tests
                    output = result.stdout
                    test_count = output.count("::test_")
                    print(f"âœ… {test_file} discovers {test_count} test functions")
                else:
                    print(f"âš ï¸  {test_file} discovery issues: {result.stderr}")
                    
            except subprocess.TimeoutExpired:
                print(f"âš ï¸  {test_file} discovery timed out")
            except Exception as e:
                print(f"âš ï¸  {test_file} discovery error: {e}")
    
    def test_individual_test_execution(self):
        """Test individual test file execution to validate they run."""
        tests_dir = Path(__file__).parent
        execution_results = {}
        
        for test_file in self.test_files:
            test_path = tests_dir / test_file
            
            print(f"\nðŸ” Testing execution of {test_file}...")
            
            try:
                # Run the test file with a short timeout
                start_time = time.time()
                result = subprocess.run([
                    sys.executable, "-m", "pytest", 
                    str(test_path), "-v", "--tb=short", "-x"
                ], 
                capture_output=True, text=True, timeout=120
                )
                end_time = time.time()
                
                execution_time = end_time - start_time
                execution_results[test_file] = {
                    'returncode': result.returncode,
                    'execution_time': execution_time,
                    'stdout_lines': len(result.stdout.split('\n')),
                    'stderr_lines': len(result.stderr.split('\n'))
                }
                
                # Analyze results
                if result.returncode == 0:
                    print(f"âœ… {test_file} executed successfully ({execution_time:.1f}s)")
                elif result.returncode == 5:  # No tests collected
                    print(f"âš ï¸  {test_file} no tests collected")
                else:
                    print(f"âš ï¸  {test_file} execution issues (code {result.returncode})")
                    
                    # Show first few lines of error for debugging
                    if result.stderr:
                        error_lines = result.stderr.split('\n')[:3]
                        for line in error_lines:
                            if line.strip():
                                print(f"   Error: {line.strip()}")
                
            except subprocess.TimeoutExpired:
                print(f"âš ï¸  {test_file} execution timed out (>120s)")
                execution_results[test_file] = {'timeout': True}
            except Exception as e:
                print(f"âš ï¸  {test_file} execution error: {e}")
                execution_results[test_file] = {'error': str(e)}
        
        # Summary of execution results
        print(f"\nðŸ“Š Test Execution Summary:")
        successful = sum(1 for r in execution_results.values() 
                        if isinstance(r, dict) and r.get('returncode') == 0)
        total = len(execution_results)
        print(f"   Successful executions: {successful}/{total}")
        
        if successful > 0:
            avg_time = sum(r.get('execution_time', 0) for r in execution_results.values() 
                          if isinstance(r, dict) and 'execution_time' in r) / successful
            print(f"   Average execution time: {avg_time:.1f}s")
    
    def test_test_coverage_analysis(self):
        """Analyze test coverage and comprehensiveness."""
        tests_dir = Path(__file__).parent
        coverage_analysis = {}
        
        for test_file in self.test_files:
            test_path = tests_dir / test_file
            
            try:
                with open(test_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                
                # Count test functions
                test_functions = content.count("def test_")
                
                # Count assertion statements
                assertions = content.count("assert ")
                
                # Count real data usage indicators
                real_data_indicators = [
                    "backup_unread", "emails_final.db", "eml_file", 
                    "real_", "actual_", "sample_", ".eml"
                ]
                real_data_usage = sum(content.count(indicator) for indicator in real_data_indicators)
                
                # Count pytest fixtures and marks
                pytest_marks = content.count("@pytest.mark")
                
                coverage_analysis[test_file] = {
                    'test_functions': test_functions,
                    'assertions': assertions,
                    'real_data_indicators': real_data_usage,
                    'pytest_marks': pytest_marks,
                    'lines_of_code': len(content.split('\n'))
                }
                
                print(f"âœ… {test_file} coverage analysis:")
                print(f"   Test functions: {test_functions}")
                print(f"   Assertions: {assertions}")
                print(f"   Real data usage: {real_data_usage} indicators")
                print(f"   Lines of code: {len(content.split('\n'))}")
                
            except Exception as e:
                print(f"âš ï¸  {test_file} coverage analysis error: {e}")
        
        # Overall coverage summary
        total_tests = sum(ca.get('test_functions', 0) for ca in coverage_analysis.values())
        total_assertions = sum(ca.get('assertions', 0) for ca in coverage_analysis.values())
        total_real_data = sum(ca.get('real_data_indicators', 0) for ca in coverage_analysis.values())
        
        print(f"\nðŸ“Š Overall Test Coverage:")
        print(f"   Total test functions: {total_tests}")
        print(f"   Total assertions: {total_assertions}")
        print(f"   Real data usage indicators: {total_real_data}")
        print(f"   Average assertions per test: {total_assertions/total_tests:.1f}" if total_tests > 0 else "   No tests found")
    
    def test_dependencies_validation(self):
        """Validate that test dependencies are available or properly handled."""
        dependencies_check = {}
        
        # Check for pytest
        try:
            import pytest
            dependencies_check['pytest'] = f"Available (version {pytest.__version__})"
        except ImportError:
            dependencies_check['pytest'] = "Missing - Required for testing"
        
        # Check for core application modules
        app_modules = [
            'core.gmail_assistant',
            'core.email_classifier',
            'parsers.advanced_email_parser'
        ]
        
        for module in app_modules:
            try:
                # Try to import from src directory
                sys.path.insert(0, str(Path(__file__).parent.parent / "src"))
                importlib.import_module(module)
                dependencies_check[module] = "Available"
            except ImportError as e:
                dependencies_check[module] = f"Import error: {str(e)[:50]}..."
        
        # Check for test data availability
        data_sources = [
            ("EML files", "backup_unread/2025/09"),
            ("Database", "data/databases/emails_final.db"),
            ("Config files", "config"),
            ("Main script", "main.py")
        ]
        
        for name, path in data_sources:
            path_obj = Path(path)
            if path_obj.exists():
                if path_obj.is_file():
                    dependencies_check[name] = f"Available ({path_obj.stat().st_size} bytes)"
                else:
                    file_count = len(list(path_obj.glob("*")))
                    dependencies_check[name] = f"Available ({file_count} items)"
            else:
                dependencies_check[name] = "Not found"
        
        # Report dependency status
        print(f"\nðŸ“‹ Dependencies Validation:")
        for dep, status in dependencies_check.items():
            status_indicator = "âœ…" if "Available" in status else "âš ï¸ "
            print(f"   {status_indicator} {dep}: {status}")
    
    def test_generate_test_execution_report(self):
        """Generate a comprehensive test execution report."""
        report = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'test_files': self.test_files,
            'environment': {
                'python_version': sys.version,
                'working_directory': str(Path.cwd()),
                'test_directory': str(Path(__file__).parent)
            }
        }
        
        # Save report to file
        report_path = Path(__file__).parent / "test_execution_report.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"âœ… Test execution report generated: {report_path}")
        print(f"   Report timestamp: {report['timestamp']}")
        print(f"   Test files covered: {len(report['test_files'])}")
        
        # Verify report file
        assert report_path.exists()
        assert report_path.stat().st_size > 0


class TestRealDataValidation:
    """Validate that real data sources are properly utilized in tests."""
    
    def test_backup_directory_usage(self):
        """Test that backup directories are properly used in tests."""
        backup_dir = Path("backup_unread")
        if not backup_dir.exists():
            pytest.skip("No backup directory available for validation")
        
        # Check for EML files
        eml_files = list(backup_dir.rglob("*.eml"))
        if eml_files:
            print(f"âœ… Found {len(eml_files)} EML files for testing")
            
            # Sample a few files for validation
            for eml_file in eml_files[:3]:
                assert eml_file.stat().st_size > 0, f"Empty EML file: {eml_file}"
                print(f"   Valid EML file: {eml_file.name} ({eml_file.stat().st_size} bytes)")
        else:
            print("âš ï¸  No EML files found in backup directory")
    
    def test_database_availability(self):
        """Test that database files are available and accessible."""
        db_path = Path("data/databases/emails_final.db")
        if not db_path.exists():
            pytest.skip("No database file available for validation")
        
        # Verify database accessibility
        import sqlite3
        try:
            conn = sqlite3.connect(str(db_path))
            cursor = conn.cursor()
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
            tables = cursor.fetchall()
            conn.close()
            
            print(f"âœ… Database accessible with {len(tables)} tables")
            if tables:
                print(f"   Tables: {[t[0] for t in tables]}")
                
        except Exception as e:
            print(f"âš ï¸  Database access error: {e}")
    
    def test_configuration_files_validation(self):
        """Test that configuration files are valid and usable."""
        config_dir = Path("config")
        if not config_dir.exists():
            pytest.skip("No config directory available for validation")
        
        json_files = list(config_dir.glob("*.json"))
        valid_configs = 0
        
        for config_file in json_files:
            try:
                with open(config_file, 'r') as f:
                    config_data = json.load(f)
                
                assert isinstance(config_data, (dict, list))
                valid_configs += 1
                print(f"âœ… Valid config: {config_file.name}")
                
            except json.JSONDecodeError as e:
                print(f"âš ï¸  Invalid JSON in {config_file.name}: {e}")
            except Exception as e:
                print(f"âš ï¸  Config validation error for {config_file.name}: {e}")
        
        if valid_configs > 0:
            print(f"âœ… {valid_configs} valid configuration files found")


if __name__ == "__main__":
    pytest.main([__file__, "-v"])